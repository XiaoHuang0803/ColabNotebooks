{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPLrsozqVYT8cvz3Bzb4M6z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/XiaoHuang0803/ColabNotebooks/blob/main/002_Working_with_text_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install tiktoken"
      ],
      "metadata": {
        "id": "82HjBbqvjQfZ"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> ### 2. Working with text data\n",
        "\n"
      ],
      "metadata": {
        "id": "0vzMK4E02sn0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Tokenizing text"
      ],
      "metadata": {
        "id": "E4TU2g2x23LM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zqXrzTMjMjI6",
        "outputId": "517942b2-e292-45b2-ee89-301a2d15ce4f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('the-verdict.txt', <http.client.HTTPMessage at 0x7ffacb1bb310>)"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "# Download the verdict text\n",
        "import urllib.request\n",
        "url = (\"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/the-verdict.txt\")\n",
        "file_path = \"the-verdict.txt\"\n",
        "urllib.request.urlretrieve(url, file_path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    raw_text = f.read()\n",
        "print(\"Total number of characters: \", len(raw_text))\n",
        "print(raw_text[:99])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aQSnnxspyev0",
        "outputId": "7081507f-82d7-43d8-beaa-ebe3764e920d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of characters:  20479\n",
            "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "text = \"Hello, world. This, is a test.\"\n",
        "result = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
        "result = [item for item in result if item.strip()]\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SaGohhYBzaR0",
        "outputId": "25268ddc-5987-4036-a76b-18d7b814f7e1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', ',', 'world', '.', 'This', ',', 'is', 'a', 'test', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
        "preprocessed = [item for item in preprocessed if item.strip()]\n",
        "print(len(preprocessed))\n",
        "print(preprocessed[:30])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8SAE8iR0Ba5",
        "outputId": "13f24d95-6a95-4fec-8001-222bc8c8a1b6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4690\n",
            "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Converting tokens into token IDs"
      ],
      "metadata": {
        "id": "FwbA4p5e21d9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_words = sorted(set(preprocessed))\n",
        "vocab_size = len(all_words)\n",
        "print(vocab_size)\n",
        "\n",
        "vocab = {token:integer for integer, token in enumerate(all_words)}\n",
        "# for i, item in enumerate(vocab.items()):\n",
        "#     print(item)\n",
        "#     if i>= 15:\n",
        "#         break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tUZKUG-e2eF0",
        "outputId": "41559ced-9df9-484f-e38d-6a2ae46b9d5e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1130\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Implement a tokenizer class\n",
        "class SimpleTokenizerV1:\n",
        "    def __init__(self, vocab):\n",
        "        self.str_to_int = vocab\n",
        "        self.int_to_str = {i:s for s,i in vocab.items()}\n",
        "\n",
        "    def encode(self, text):\n",
        "        preprocessed = re.split(r'([,.?_!\"(\\')]|--|\\s)', text)\n",
        "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "        ids = [self.str_to_int[s] for s in preprocessed]\n",
        "        return ids\n",
        "\n",
        "    def decode(self, ids):\n",
        "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
        "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
        "        return text"
      ],
      "metadata": {
        "id": "P0OBTbmZ3OKn"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = SimpleTokenizerV1(vocab)\n",
        "text = \"\"\"\n",
        "\"It's the last he painted, you know,\"\n",
        "Mrs. Gisburn said with pardonable pride.\n",
        "\"\"\"\n",
        "ids = tokenizer.encode(text)\n",
        "print(ids)\n",
        "print(tokenizer.decode(ids))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yyFHM6lWTwkU",
        "outputId": "0d57b0a4-d77e-4222-aba1-848af35cfe12"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 56, 2, 850, 988, 602, 533, 746, 5, 1126, 596, 5, 1, 67, 7, 38, 851, 1108, 754, 793, 7]\n",
            "\" It' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Adding special context tokens"
      ],
      "metadata": {
        "id": "ZkGVkCxFWWfl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_tokens = sorted(list(set(preprocessed)))\n",
        "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
        "vocab = {token:integer for integer, token in enumerate(all_tokens)}"
      ],
      "metadata": {
        "id": "rB4o-ztmUc2S"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_tokens[-10:]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wcgGIq1ehcGg",
        "outputId": "8ae58d04-685b-4d8a-c577-e3bb01510411"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['year',\n",
              " 'years',\n",
              " 'yellow',\n",
              " 'yet',\n",
              " 'you',\n",
              " 'younger',\n",
              " 'your',\n",
              " 'yourself',\n",
              " '<|endoftext|>',\n",
              " '<|unk|>']"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleTokenizerV2:\n",
        "    def __init__(self, vocab):\n",
        "        self.str_to_int = vocab\n",
        "        self.int_to_str = {i:s for s,i in vocab.items()}\n",
        "\n",
        "    def encode(self, text):\n",
        "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
        "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "        preprocessed = [item if item in self.str_to_int else \"<|unk|>\" for item in preprocessed]\n",
        "        ids = [self.str_to_int[s] for s in preprocessed]\n",
        "        return ids\n",
        "\n",
        "    def decode(self, ids):\n",
        "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
        "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
        "        return text"
      ],
      "metadata": {
        "id": "gJwzk_ujd1CI"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text1 = \"Hello, do you line tea?\"\n",
        "text2 = \"In the sunlit terrances of the palace.\"\n",
        "text = \" <|endoftext|> \".join((text1, text2))\n",
        "print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YhQPdeaNeeSP",
        "outputId": "dc8771d0-403b-45aa-9bab-3e31db594b12"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello, do you line tea? <|endoftext|> In the sunlit terrances of the palace.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = SimpleTokenizerV2(vocab)\n",
        "print(tokenizer.encode(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HQslIJ79gnoo",
        "outputId": "b4595198-1855-4148-a4be-598ec5a86d10"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1131, 5, 355, 1126, 630, 975, 10, 1130, 55, 988, 956, 1131, 722, 988, 1131, 7]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.decode(tokenizer.encode(text)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CI2shhBSg5cw",
        "outputId": "32805941-9481-4729-b629-fcf2fb4ea870"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|unk|>, do you line tea? <|endoftext|> In the sunlit <|unk|> of the <|unk|>.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "McXgKXcMg9IA"
      },
      "execution_count": 29,
      "outputs": []
    }
  ]
}